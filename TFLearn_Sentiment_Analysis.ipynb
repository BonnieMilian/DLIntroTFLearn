{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning: Introducción con TFLearn\n",
    "## Sentiment analysis con TFLearn\n",
    "\n",
    "Vamos a construir una Red Neuronal para aplicar Sentiment Analysis en review de peliculas. Vamos a utilizar [TFLearn](http://tflearn.org/), que es una librería de alto nivel contruida por encima de TensorFlow. TFLearn vuelve simple el contruir Redes Neuronales ya que solo se definen los layers. Se encarga de la mayoria de detalles por uno.\n",
    "\n",
    "\n",
    "Comenzamos importando todos los módulos que vamos a utilzar, luego cargaremos la data y la prepararemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparamos la data\n",
    "\n",
    "Nuestro objetivo aquí es convertir los reviews en un vector de palabras. El vector de palabras tendrá elementos representantes del vocabulario total. Por ejemplo si la segunda posición del vector representa la palabra 'the', para cada review contaremos el número de veces que aparece 'the' y colocaremos ese valor en el vector. Les mostraré ejemplos conforme construimos el Input Data de las reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer u obtener la data\n",
    "\n",
    "Usaremos la librería pandas para leer los reviews y sus etiquetas (labels) positiva/negativa de un archivo coma separado.  Los datos que estamos utilizando ya han sido preprocesados un poco y sabemos que sólo utiliza caracteres en minúsculas. Si tubieras datos sin pre procesar aquí abría que añadir un paso de limpiar los datos. Con ello solucionamos diferentes variaciones de una misma palabra por ejemplo 'the', 'The', 'tHe' y 'THE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de la frecuencia de las  palabras\n",
    "\n",
    "Para comenzar debemos contar la frecuencia con que cada palabra aparece en los datos. Usaremos este recuento para crear un vocabulario que usaremos para codificar las reviews. A ese resultado se le llama Saco de Palabras ([bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model)). Lo utilizaremos para seleccionar el vocabulario y construir nuestro vector de palabras. Trataremos de implementarlo utilizando un [Counter class](https://docs.python.org/2/library/collections.html#collections.Counter).\n",
    "\n",
    "> **Ejercicio:** Crear un bag of words de las reviews y asignarlo a `total_counts`. Las reviews se almacenan en el `reviews` [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html). Si deseas las reviews como un array de Numpy, utiliza `reviews.values`. Puedes iterar las filas en el DataFrame con `for idx, row in reviews.iterrows():` ([documentación](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html)). Cuando dividas las reviews en palabras, usa `.split(' ')` y no `.split()` para que nos salga el mismo resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_counts = # bag of words here\n",
    "\n",
    "print(\"Total words in data set: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mantengamos las 10000 primeras palabras más frecuentes. La mayoría de las palabras del vocabulario se usan raramente por que tendrán poco efecto en nuestras predicciones. A continuación, ordenaremos `vocab` por conteo y solo usaremos las 10000 más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)[:10000]\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál es la última palabra en nuestro vocabulario? Podemos usar esto para juzgar si 10000 es demasiado pequeño. Si la última palabra es bastante común, es probable que tengamos que guardar más palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(vocab[-1], ': ', total_counts[vocab[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última palabra en nuestro vocabulario se muestra en 30 reviews de 25000. Creo que es justo decir que esto es una pequeña proporción de comentarios. Probablemente estamos bien con este número de palabras.\n",
    "\n",
    "**Nota:** Cuando lo ejecutes, puede que veas una palabra diferente, pero sigue siendo de valor `30`. Eso es porque hay varias palabras con la misma cantidad de ocurrencias, y la `Counter` class no garantiza cual va a retornar en ese caso.\n",
    "\n",
    "Ahora para cada review, crearemos un vector de palabras. Primero necesitamos hacer un mapeo de palabras a un índice, bastante fácil de hacer con una comprensión del diccionario.\n",
    "\n",
    "> **Ejercicio:** Crear un dictionario llamado `word2idx` que asigne cada palabra del vocabulario un índice. La primera palabra en `vocab` tiene el índice `0`, la segunda palabra tiene índice `1`, y así sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = ## create the word-to-index dictionary here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto a vector, función\n",
    "\n",
    "Ahora podemos escribir una función que convierta un texto a un vector de palabras. La función recibe un string de palabras y retorna un vector con el conteo de palabras. Un algoritmo genérico para hacer eso:\n",
    "\n",
    "* Inicializa el vector de palabras con [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html), debería ser del largo del vocabulario.\n",
    "* Separa el input string del texto en una lista de palabras con `.split(' ')`.\n",
    "* Por cada palbra en esa lista, incrementa el elemento en el índice asociado a esa palabra, que lo obtienes de `word2idx`.\n",
    "\n",
    "**Nota:** Como no todas las palabras que apareazcan están en el diccionario `vocab`, tendrás un error de key \"index out of bounds\", si se trata de acceder a una palabra que no esté. Puedes usar el método `.get` del diccionario `word2idx` para especificar el valor default de retorno cuando de un error. Por ejemplo, `word2idx.get(word, None)` retorna `None` si `word` no existe en el diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo hiciste bien el siguiente código retornará:\n",
    "\n",
    "```\n",
    "text_to_vector('The tea is for a party to celebrate '\n",
    "               'the movie so she has no time for a cake')[:65]\n",
    "                   \n",
    "array([0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
    "```       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_to_vector('The tea is for a party to celebrate '\n",
    "               'the movie so she has no time for a cake')[:65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, itera en todas las reviews y conviertelas en un vector de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((len(reviews), len(vocab)), dtype=np.int_)\n",
    "for ii, (_, text) in enumerate(reviews.iterrows()):\n",
    "    word_vectors[ii] = text_to_vector(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing out the first 5 word vectors\n",
    "word_vectors[:5, :23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation, Test sets\n",
    "\n",
    "Ahora que tenemos los vectores de palabras, estamos listos para dividir nuestros datos en train, validation y test sets. Utilizaremos para entrenar el train set, setearemos los hyperparameters con el validation set, y hasta el final evaluaremos el performace de la red neuronal con el test set. \n",
    "Aquí estamos usando la función `to_categorical` de TFLearn para remodelar los datos objetivo para que tengamos 2 unidades de output, para podamos clasificar utilizando una función de activación softmax. Nosotros no crearemos el set de validación eso lo hara TFLearn por nosotros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = (labels=='positive').astype(np.int_)\n",
    "records = len(labels)\n",
    "\n",
    "shuffle = np.arange(records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "trainX, trainY = word_vectors[train_split,:], to_categorical(Y.values[train_split], 2)\n",
    "testX, testY = word_vectors[test_split,:], to_categorical(Y.values[test_split], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo la Red Neuronal\n",
    "\n",
    "[TFLearn](http://tflearn.org/) te permite construir la Red  [definiendo los layers](http://tflearn.org/layers/core/). \n",
    "\n",
    "### Input layer\n",
    "\n",
    "Para el input layer, solo necesitamos decirle cuantas neuronas tendrá. Por ejemplo,\n",
    "```\n",
    "net = tflearn.input_data([None, 100])\n",
    "```\n",
    "esto creará una red con 100 neuronas en el input layer. El primer elemento de la lista, `None` en este caso, setea el tamaño del batch.\n",
    "\n",
    "El número de inputs a la Red tiene que ser del tamaño de nuestra data, por ejemplo si tenemos 5000 palabras en nuestro vector de vocabulario entonces las neuronas de entrada deben ser 5000.\n",
    "\n",
    "\n",
    "### Añadir layers\n",
    "\n",
    "Para agregar nuevas \n",
    "To add new hidden layers, you use \n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, n_units, activation='ReLU')\n",
    "```\n",
    "\n",
    "This adds a fully connected layer where every unit in the previous layer is connected to every unit in this layer. The first argument `net` is the network you created in the `tflearn.input_data` call. It's telling the network to use the output of the previous layer as the input to this layer. You can set the number of units in the layer with `n_units`, and set the activation function with the `activation` keyword. You can keep adding layers to your network by repeated calling `net = tflearn.fully_connected(net, n_units)`.\n",
    "\n",
    "### Output layer\n",
    "\n",
    "The last layer you add is used as the output layer. Therefore, you need to set the number of units to match the target data. In this case we are predicting two classes, positive or negative sentiment. You also need to set the activation function so it's appropriate for your model. Again, we're trying to predict if some input data belongs to one of two classes, so we should use softmax.\n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "```\n",
    "\n",
    "### Training\n",
    "To set how you train the network, use \n",
    "\n",
    "```\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "Again, this is passing in the network you've been building. The keywords: \n",
    "\n",
    "* `optimizer` sets the training method, here stochastic gradient descent\n",
    "* `learning_rate` is the learning rate\n",
    "* `loss` determines how the network error is calculated. In this example, with the categorical cross-entropy.\n",
    "\n",
    "Finally you put all this together to create the model with `tflearn.DNN(net)`. So it ends up looking something like \n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, 10])                          # Input\n",
    "net = tflearn.fully_connected(net, 5, activation='ReLU')      # Hidden\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')   # Output\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(net)\n",
    "```\n",
    "\n",
    "> **Exercise:** Below in the `build_model()` function, you'll put together the network using TFLearn. You get to choose how many layers to use, how many hidden units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network building\n",
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #### Your code ####\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing the model\n",
    "\n",
    "Next we need to call the `build_model()` function to actually build the model. In my solution I haven't included any arguments to the function, but you can add arguments so you can change parameters in the model if you want.\n",
    "\n",
    "> **Note:** You might get a bunch of warnings here. TFLearn uses a lot of deprecated code in TensorFlow. Hopefully it gets updated to the new TensorFlow version soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Now that we've constructed the network, saved as the variable `model`, we can fit it to the data. Here we use the `model.fit` method. You pass in the training features `trainX` and the training targets `trainY`. Below I set `validation_set=0.1` which reserves 10% of the data set as the validation set. You can also set the batch size and number of epochs with the `batch_size` and `n_epoch` keywords, respectively. Below is the code to fit our the network to our word vectors.\n",
    "\n",
    "You can rerun `model.fit` to train the network further if you think you can increase the validation accuracy. Remember, all hyperparameter adjustments must be done using the validation set. **Only use the test set after you're completely done training the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=128, n_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "After you're satisified with your hyperparameters, you can run the network on the test set to measure its performance. Remember, *only do this after finalizing the hyperparameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == testY[:,0], axis=0)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out your own text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function that uses your model to predict sentiment\n",
    "def test_sentence(sentence):\n",
    "    positive_prob = model.predict([text_to_vector(sentence.lower())])[0][1]\n",
    "    print('Sentence: {}'.format(sentence))\n",
    "    print('P(positive) = {:.3f} :'.format(positive_prob), \n",
    "          'Positive' if positive_prob > 0.5 else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"Moonlight is by far the best movie of 2016.\"\n",
    "test_sentence(sentence)\n",
    "\n",
    "sentence = \"It's amazing anyone could be talented enough to make something this spectacularly awful\"\n",
    "test_sentence(sentence)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
